{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style='color:Red'> Social Media News Generation </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color:Blue'>Project By: </span>\n",
    "### Nisha Keshav Shenvi\n",
    "### Nimisha Sharma\n",
    "### Ravella.Puthali\n",
    "### Simna Ashraf\n",
    "### Sneha Shankar Hirnaik\n",
    "\n",
    "## <span style='color:Blue'> Mentor:</span>\n",
    "### Alarsh Tiwari "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Note: Create a folder named Cluster Output in the same location where this notebook is .\n",
    "***\n",
    "***\n",
    "***\n",
    "#### <span style='background :yellow' > Importing libraries</span> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import xlsxwriter\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.cluster import KMeans\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style='background :yellow' > Accessing files</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.abspath(os.path.dirname('__file__'))\n",
    "url = os.path.join(path, 'bbchealth.txt')\n",
    "f = open(url, \"r\", encoding=\"utf8\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style='background :yellow' >Declaration of the variables </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = list(f)\n",
    "Headlines_List = []\n",
    "pointer = 1\n",
    "num1 = 0\n",
    "sr_num = 0\n",
    "# file that will have final result\n",
    "url1 = os.path.join(path, 'Cluster_Output\\\\')\n",
    "finalresult = url1+'cluster_output.xlsx'\n",
    "#check if output file already exists and delete it\n",
    "if os.path.exists(finalresult):\n",
    "    os.remove(finalresult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style='background :yellow' >Data Preprocessing </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(tweets)):\n",
    "\n",
    "        # remove \\n from the end after every sentence\n",
    "        tweets[i] = tweets[i].strip('\\n')\n",
    "\n",
    "        # Remove the tweet id and timestamp\n",
    "        tweets[i] = tweets[i][50:]\n",
    "       \n",
    "        # Remove any URL\n",
    "        tweets[i] = re.sub(r\"http\\S+\", \"\", tweets[i])\n",
    "        tweets[i] = re.sub(r\"www\\S+\", \"\", tweets[i])\n",
    "       \n",
    "        # Remove any hash-tags symbols\n",
    "        tweets[i] = tweets[i].replace('#', '')\n",
    "\n",
    "        # Convert every word to lowercase\n",
    "        tweets[i] = tweets[i].lower()\n",
    "\n",
    "        # remove punctuations\n",
    "        tweets[i] = tweets[i].translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "        # trim extra spaces\n",
    "        tweets[i] = \" \".join(tweets[i].split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style='background :yellow' > Creating a Dataframe to store prefrocessed tweets </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DataFrame (tweets,columns=['Headlines'])\n",
    "dataframe_real = DataFrame (tweets,columns=['Headlines'])\n",
    "output_df = DataFrame (tweets,columns=['Headlines'])\n",
    "ac_df = DataFrame (tweets,columns=['Headlines'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <span style='background :yellow' >Feature Extracting using TfidfVectorizer </span>\n",
    "##### The TF-IDF (term frequency-inverse document frequency) algorithm is based on word statistics for text feature extraction. ... The representation of words needs to extract the similarity of words, and the similarity among words needs to be obtained by the meaning of words in texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "#Fit(): Method calculates the parameters μ and σ and saves them as internal objects.\n",
    "#Transform(): Method using these calculated parameters apply the transformation to a particular dataset. \n",
    "#Fit_transform(): joins the fit() and transform() method for transformation of dataset\n",
    "X = vectorizer.fit_transform(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style='background :yellow' > Settings for final result in Excel </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workbook = xlsxwriter.Workbook(finalresult)\n",
    "worksheet = workbook.add_worksheet()\n",
    "worksheet.set_column('C:C', 50)\n",
    "bold = workbook.add_format({'bold': True, 'font_color': \"White\", 'font_size': 14, 'center_across': True, 'border': True })\n",
    "bold.set_font_color('White')\n",
    "bold.set_bg_color('Magenta')\n",
    "all_cells = workbook.add_format({'bold': False, 'font_size': 13, 'reading_order': True, 'border': True, 'center_across': True, })\n",
    "last_column = workbook.add_format({'bold': False, 'font_size': 13, 'reading_order': True, 'border': True, 'center_across': True })\n",
    "last_column.set_text_wrap()\n",
    "worksheet.write('A1', 'Sr. No.', bold)\n",
    "worksheet.write('B1', 'Domain', bold)\n",
    "worksheet.write('C1', 'News', bold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = df['Headlines'].count()\n",
    "#print(num)\n",
    "true_k = (int)(num/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style='background :yellow' > K-Mean Clustering </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-means\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "df[\"cluster\"] = model.fit_predict(X)\n",
    "cluster_details = model.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style='background :yellow' >Agglomerative Hierarchical Clustering</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_true_k = (int)((len(cluster_details))/2)\n",
    "df[\"Agglomerative_Clustering\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agglomerative Clustering\n",
    "#affinity methods:“euclidean”, “l1”, “l2”, “manhattan”, “cosine”, or “precomputed”\n",
    "hc = AgglomerativeClustering(n_clusters = ac_true_k, affinity = 'euclidean', linkage ='ward')\n",
    "df[\"Agglomerative_Clustering\"] = hc.fit_predict(cluster_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### <span style='color:Green'>**Main :**</span>\n",
    "##### > Agglomerative_Clustering,\n",
    "##### > Cosine Similarity\n",
    "##### > Finding Domains of the Cluster\n",
    "##### > Displaying News in the Excel sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(0, ac_true_k):\n",
    "    ac_num = 'Agglomerative_Clustering == '+str(k)\n",
    "    output_str = 'Cluster '+str(k)+'\\\\'\n",
    "    output_url = os.path.join(url1, output_str)\n",
    "    ac_url = output_url+'Bifurcated News '+str(k)+'.txt'\n",
    "    if os.path.exists(output_url):\n",
    "        shutil.rmtree(output_url)\n",
    "    os.mkdir(output_url)\n",
    "    file_name = output_url+'News.txt'\n",
    "    ac_df = df.query(ac_num)\n",
    "#print(dataframe_real)\n",
    "    Headlines_List = ac_df['Headlines'].values.tolist()\n",
    "    for a in range(0, len(Headlines_List)):\n",
    "        Headlines_List[a]+=\". \"\n",
    "\n",
    "#Checking for Cosine Similarity in Agglomerative Clusters\n",
    "    for i in range(len(Headlines_List)):\n",
    "   # print('\\n\\n\\n',Headlines_List[X])\n",
    "        j=i+1\n",
    "        while j<len(Headlines_List):\n",
    "            #+print(i,\"\\t\",j,\"\\t\",len(Headlines_List))\n",
    "       \n",
    "            X=Headlines_List[i]\n",
    "            Y=Headlines_List[j]\n",
    "       \n",
    "            #print(X,\"\\n\",Y)\n",
    "       \n",
    "            X_list = word_tokenize(X)\n",
    "            Y_list = word_tokenize(Y)\n",
    "       \n",
    "        # sw contains the list of stopwords\n",
    "            sw = stopwords.words('english')\n",
    "            l1 =[];l2 =[]\n",
    "\n",
    "        # remove stop words from string\n",
    "            X_set = {w for w in X_list if not w in sw}\n",
    "            Y_set = {w for w in Y_list if not w in sw}\n",
    "        # form a set containing keywords of both strings\n",
    "            rvector = X_set.union(Y_set)\n",
    "            for w in rvector:\n",
    "                if w in X_set: l1.append(1) # create a vector\n",
    "                else: l1.append(0)\n",
    "                if w in Y_set: l2.append(1)\n",
    "                else: l2.append(0)\n",
    "               \n",
    "            c = 0\n",
    "\n",
    "        # cosine formula\n",
    "            for z in range(len(rvector)):\n",
    "                c+= l1[z]*l2[z]\n",
    "            cosine = c / float((sum(l1)*sum(l2))**0.5)\n",
    "            if(cosine>0.7):\n",
    "                del Headlines_List[j]\n",
    "       \n",
    "            j=j+1\n",
    "            #end of while loop\n",
    "   \n",
    "    num = 0\n",
    "    for s in Headlines_List:\n",
    "            num += 1\n",
    "            with open(file_name, \"a\") as text_file:\n",
    "                text_file.write(\"News \"+str(num)+\": \"+s+\"\\n\")\n",
    "                text_file.close()\n",
    "               \n",
    "   \n",
    "   \n",
    "    dataframe_real = pd.DataFrame(Headlines_List, columns=['Headlines'])\n",
    "    lines_List = dataframe_real['Headlines'].values.tolist()\n",
    "    dataframe_real['cluster'] = \"\"\n",
    "   \n",
    "    Z = vectorizer.fit_transform(lines_List)\n",
    "    terms = vectorizer.get_feature_names()\n",
    "   \n",
    "    #code to find News Domain\n",
    "    def count(fname, words_list):\n",
    "        full_text = fname\n",
    "        count_result = dict()\n",
    "        for word in words_list:\n",
    "            for text in full_text:\n",
    "                if word in count_result:\n",
    "                    count_result[word] = count_result[word] + text.count(word)\n",
    "                else:\n",
    "                    count_result[word] = text.count(word)\n",
    "        return count_result\n",
    "   \n",
    "\n",
    "\n",
    "    counter = count(lines_List, terms)\n",
    "    max = 0\n",
    "    word = \"\"\n",
    "    for i in counter:\n",
    "        if(max<counter[i]):\n",
    "            max=counter[i]\n",
    "            word=i\n",
    "   \n",
    "    for i in range(len(dataframe_real.index)):\n",
    "        dataframe_real.iat[i, 1]=word\n",
    "    cluster_list=dataframe_real['cluster'].values.tolist()\n",
    "   \n",
    "    o=0\n",
    "  # Writing data into Result Excel sheet \n",
    "    for item in range(len(dataframe_real.index)):\n",
    "        sr_num += 1\n",
    "        worksheet.write(item+pointer, 0, sr_num, all_cells)\n",
    "        worksheet.write(item+pointer, 1, cluster_list[item], all_cells)\n",
    "        worksheet.write(item+pointer, 2, lines_List[item], last_column)  \n",
    "        o=item  \n",
    "    pointer += o+1\n",
    "\n",
    "\n",
    "workbook.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task Completed!\n"
     ]
    }
   ],
   "source": [
    "print(\"Task Completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>>>>>>> #  <span style='color:Yellow'><span style='background :Blue'> ***Thank You*** </span></span> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
